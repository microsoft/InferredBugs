        public static void TrainWithSelu(string dataFilesPath)
        {
            using (var lap = BrightWireProvider.CreateLinearAlgebra()) {
                var graph = new GraphFactory(lap);

                // parse the iris CSV into a data table and normalise
                var dataTable = new StreamReader(new MemoryStream(File.ReadAllBytes(dataFilesPath))).ParseCSV(',').Normalise(NormalisationType.Standard);

                // split the data table into training and test tables
                var split = dataTable.Split(0);
                var trainingData = graph.CreateDataSource(split.Training);
                var testData = graph.CreateDataSource(split.Test);

                // one hot encoding uses the index of the output vector's maximum value as the classification label
                var errorMetric = graph.ErrorMetric.OneHotEncoding;

                // configure the network properties
                graph.CurrentPropertySet
                    .Use(graph.GradientDescent.RmsProp)
                    .Use(graph.GaussianWeightInitialisation(true, 0.1f, GaussianVarianceCalibration.SquareRoot2N, GaussianVarianceCount.FanInFanOut))
                ;

                // create the training engine and schedule a training rate change
                const float TRAINING_RATE = 0.1f;
                var engine = graph.CreateTrainingEngine(trainingData, TRAINING_RATE, batchSize: 32);

	            const int LAYER_SIZE = 32;

	            Func<INode> activation = () => new SeluActivation();
	            //Func<INode> activation = () => graph.ReluActivation();

                // create the network with the custom activation function
				graph.Connect(engine)
				    .AddFeedForward(LAYER_SIZE)
				    .AddBatchNormalisation()
				    .Add(activation())
				    .AddFeedForward(LAYER_SIZE)
				    .AddBatchNormalisation()
					.Add(activation())
				    .AddFeedForward(LAYER_SIZE)
				    .AddBatchNormalisation()
				    .Add(activation())
					.AddFeedForward(LAYER_SIZE)
					.AddBatchNormalisation()
					.Add(activation())
					.AddFeedForward(LAYER_SIZE)
					.AddBatchNormalisation()
					.Add(activation())
					.AddFeedForward(LAYER_SIZE)
					.AddBatchNormalisation()
					.Add(activation())
				    .AddFeedForward(trainingData.OutputSize)
				    .Add(graph.SoftMaxActivation())
				    .AddBackpropagation(errorMetric)
				;

                const int TRAINING_ITERATIONS = 200;
                engine.Train(TRAINING_ITERATIONS, testData, errorMetric);
            }
        }