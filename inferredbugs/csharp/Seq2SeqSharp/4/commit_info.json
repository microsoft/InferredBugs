{
    "hash": "96f803819a55d0e71d2c0ff9ac037e7db69ef396",
    "message": "1. Fix missing ReluD operator for CPU\n2. Split and rename WordVectorSize to SrcEmbeddingDim and TgtEmbeddingDim",
    "file_num_lines": 521,
    "diff_parsed": {
        "added": [
            [
                75,
                "        public AttentionSeq2Seq(int srcEmbeddingDim, int tgtEmbeddingDim, int hiddenDim, int encoderLayerDepth, int decoderLayerDepth, Vocab vocab, string srcEmbeddingFilePath, string tgtEmbeddingFilePath,"
            ],
            [
                81,
                "            m_modelMetaData = new Seq2SeqModelMetaData(hiddenDim, srcEmbeddingDim, tgtEmbeddingDim, encoderLayerDepth, decoderLayerDepth, multiHeadNum, encoderType, decoderType, vocab, enableCoverageModel);"
            ],
            [
                123,
                "                    new BiEncoder(\"BiLSTMEncoder\", modelMetaData.HiddenDim, modelMetaData.SrcEmbeddingDim, modelMetaData.EncoderLayerDepth, raDeviceIds.GetNextItem(), isTrainable: m_isEncoderTrainable), DeviceIds);"
            ],
            [
                130,
                "                    new TransformerEncoder(\"TransformerEncoder\", modelMetaData.MultiHeadNum, modelMetaData.HiddenDim, modelMetaData.SrcEmbeddingDim, modelMetaData.EncoderLayerDepth, m_dropoutRatio, raDeviceIds.GetNextItem(),"
            ],
            [
                139,
                "                     new AttentionDecoder(\"AttnLSTMDecoder\", modelMetaData.HiddenDim, modelMetaData.TgtEmbeddingDim, contextDim,"
            ],
            [
                145,
                "                    new TransformerDecoder(\"TransformerDecoder\", modelMetaData.MultiHeadNum, modelMetaData.HiddenDim, modelMetaData.TgtEmbeddingDim, modelMetaData.Vocab.TargetWordSize, modelMetaData.EncoderLayerDepth, m_dropoutRatio, raDeviceIds.GetNextItem(),"
            ],
            [
                158,
                "            Logger.WriteLine($\"Creating embeddings for source side. Shape = '({modelMetaData.Vocab.SourceWordSize} ,{modelMetaData.SrcEmbeddingDim})'\");"
            ],
            [
                159,
                "            m_srcEmbedding = new MultiProcessorNetworkWrapper<IWeightTensor>(new WeightTensor(new long[2] { modelMetaData.Vocab.SourceWordSize, modelMetaData.SrcEmbeddingDim }, raDeviceIds.GetNextItem(), normal: NormType.Uniform, fanOut: false, name: \"SrcEmbeddings\", isTrainable: m_isSrcEmbTrainable), DeviceIds);"
            ],
            [
                160,
                ""
            ],
            [
                161,
                "            Logger.WriteLine($\"Creating embeddings for target side. Shape = '({modelMetaData.Vocab.TargetWordSize} ,{modelMetaData.TgtEmbeddingDim})'\");"
            ],
            [
                162,
                "            m_tgtEmbedding = new MultiProcessorNetworkWrapper<IWeightTensor>(new WeightTensor(new long[2] { modelMetaData.Vocab.TargetWordSize, modelMetaData.TgtEmbeddingDim }, raDeviceIds.GetNextItem(), normal: NormType.Uniform, fanOut: false, name: \"TgtEmbeddings\", isTrainable: m_isTgtEmbTrainable), DeviceIds);"
            ],
            [
                418,
                "                    using (IWeightTensor probs = g.Softmax(decOutput, runGradients: false, inPlace: true))"
            ]
        ],
        "deleted": [
            [
                75,
                "        public AttentionSeq2Seq(int embeddingDim, int hiddenDim, int encoderLayerDepth, int decoderLayerDepth, Vocab vocab, string srcEmbeddingFilePath, string tgtEmbeddingFilePath,"
            ],
            [
                81,
                "            m_modelMetaData = new Seq2SeqModelMetaData(hiddenDim, embeddingDim, encoderLayerDepth, decoderLayerDepth, multiHeadNum, encoderType, decoderType, vocab, enableCoverageModel);"
            ],
            [
                123,
                "                    new BiEncoder(\"BiLSTMEncoder\", modelMetaData.HiddenDim, modelMetaData.EmbeddingDim, modelMetaData.EncoderLayerDepth, raDeviceIds.GetNextItem(), isTrainable: m_isEncoderTrainable), DeviceIds);"
            ],
            [
                130,
                "                    new TransformerEncoder(\"TransformerEncoder\", modelMetaData.MultiHeadNum, modelMetaData.HiddenDim, modelMetaData.EmbeddingDim, modelMetaData.EncoderLayerDepth, m_dropoutRatio, raDeviceIds.GetNextItem(),"
            ],
            [
                139,
                "                     new AttentionDecoder(\"AttnLSTMDecoder\", modelMetaData.HiddenDim, modelMetaData.EmbeddingDim, contextDim,"
            ],
            [
                145,
                "                    new TransformerDecoder(\"TransformerDecoder\", modelMetaData.MultiHeadNum, contextDim, contextDim, modelMetaData.Vocab.TargetWordSize, modelMetaData.EncoderLayerDepth, m_dropoutRatio, raDeviceIds.GetNextItem(),"
            ],
            [
                158,
                "            m_srcEmbedding = new MultiProcessorNetworkWrapper<IWeightTensor>(new WeightTensor(new long[2] { modelMetaData.Vocab.SourceWordSize, modelMetaData.EmbeddingDim }, raDeviceIds.GetNextItem(), normal: NormType.Uniform, fanOut: false, name: \"SrcEmbeddings\", isTrainable: m_isSrcEmbTrainable), DeviceIds);"
            ],
            [
                159,
                "            m_tgtEmbedding = new MultiProcessorNetworkWrapper<IWeightTensor>(new WeightTensor(new long[2] { modelMetaData.Vocab.TargetWordSize, contextDim }, raDeviceIds.GetNextItem(), normal: NormType.Uniform, fanOut: false, name: \"TgtEmbeddings\", isTrainable: m_isTgtEmbTrainable), DeviceIds);"
            ],
            [
                415,
                "                    using (IWeightTensor probs = g.Softmax(decOutput, runGradients: false, inPlace: false))"
            ]
        ]
    },
    "num_lines_added": 12,
    "num_lines_removed": 9
}