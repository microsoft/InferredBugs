{
    "hash": "6a0f700b370d4c860b41b9a667bbd6f9d135a4ff",
    "message": "perf - tokenizer improvements",
    "file_num_lines": 326,
    "diff_parsed": {
        "added": [
            [
                223,
                "            T currentToken = null;"
            ],
            [
                247,
                "                    Tokenize_Whitespace(input, ref currentIndex, ref currentCharacter, currentLine, ref currentColumn, ref currentToken, tokens);"
            ],
            [
                253,
                "                    Tokenize_EscapeCharacter(input, ref currentIndex, ref currentCharacter, currentLine, ref currentColumn, ref currentToken, tokens);"
            ],
            [
                257,
                "                    Tokenize_Plain(input, ref currentIndex, ref currentCharacter, currentLine, ref currentColumn, ref currentToken, tokens);"
            ],
            [
                261,
                "                    Tokenize_DelimiterCharacter(input, ref currentIndex, ref currentCharacter, currentLine, ref currentColumn, ref currentToken, tokens);"
            ],
            [
                265,
                "                    Tokenize_Whitespace(input, ref currentIndex, ref currentCharacter, currentLine, ref currentColumn, ref currentToken, tokens);"
            ],
            [
                269,
                "                    Tokenize_Plain(input, ref currentIndex, ref currentCharacter, currentLine, ref currentColumn, ref currentToken, tokens);"
            ],
            [
                302,
                "        private void Tokenize_EscapeCharacter(string input, ref int currentIndex, ref char currentCharacter, int line, ref int col, ref T currentToken, List<T> tokens)"
            ],
            [
                317,
                "        private void Tokenize_Plain(string input, ref int currentIndex, ref char currentCharacter, int line, ref int col, ref T currentToken, List<T> tokens)"
            ],
            [
                367,
                "                            currentToken = TokenFactory(delimiter, prevToken.StartIndex + prevToken.Value.Length, prevToken.Line, prevToken.Column + prevToken.Value.Length);"
            ],
            [
                375,
                "        private void Tokenize_DelimiterCharacter(string input, ref int currentIndex, ref char currentCharacter, int line, ref int col, ref T currentToken, List<T> tokens)"
            ],
            [
                382,
                "        private void Tokenize_Whitespace(string input, ref int currentIndex, ref char currentCharacter, int line, ref int col, ref T currentToken, List<T> tokens)"
            ],
            [
                404,
                "        private void FinalizeTokenIfNotNull(ref T currentToken, List<T> tokens)"
            ],
            [
                408,
                "                tokens.Add(currentToken);"
            ],
            [
                413,
                ""
            ],
            [
                415,
                "        private void AppendToTokenSafe(ref T currentToken, char toAppend, int startIndex, int line, int col)"
            ],
            [
                458,
                "        protected virtual T TokenFactory(string currentCharacter, int currentIndex, int line, int col)"
            ],
            [
                460,
                "            var ret = (T)Activator.CreateInstance(typeof(T), currentCharacter, currentIndex, line, col);"
            ],
            [
                465,
                "        private T CreateTokenForTokenizer(char currentCharacter,int currentIndex, int line, int col)"
            ],
            [
                467,
                "            return TokenFactory(currentCharacter + \"\", currentIndex, line, col);"
            ]
        ],
        "deleted": [
            [
                134,
                ""
            ],
            [
                135,
                "        /// <summary>"
            ],
            [
                136,
                "        /// Creates a new instance of the strongly typed token and copies all of the base token's properties to the new value"
            ],
            [
                137,
                "        /// </summary>"
            ],
            [
                138,
                "        /// <typeparam name=\"T\">The type of the derived token.</typeparam>"
            ],
            [
                139,
                "        /// <returns>The strongly typed token</returns>"
            ],
            [
                140,
                "        public T As<T>() where T : Token"
            ],
            [
                141,
                "        {"
            ],
            [
                142,
                "            var ret = (T)Activator.CreateInstance(typeof(T), this.Value, this.StartIndex, this.Line, this.Column);"
            ],
            [
                143,
                "            ret.SourceFileLocation = this.SourceFileLocation;"
            ],
            [
                144,
                "            return ret;"
            ],
            [
                145,
                "        }"
            ],
            [
                195,
                "        /// <summary>"
            ],
            [
                196,
                "        /// A function that given a plain token can transform it into the strongly typed token"
            ],
            [
                197,
                "        /// </summary>"
            ],
            [
                198,
                "        public Func<Token, List<T>, T> TokenFactory { get; set; }"
            ],
            [
                199,
                ""
            ],
            [
                229,
                "            if (typeof(T) == typeof(Token))"
            ],
            [
                230,
                "            {"
            ],
            [
                231,
                "                this.TokenFactory = (token, previousTokens) => (T)token;"
            ],
            [
                232,
                "            }"
            ],
            [
                233,
                "            else"
            ],
            [
                234,
                "            {"
            ],
            [
                235,
                "                this.TokenFactory = (token, previousTokens) => token.As<T>();"
            ],
            [
                236,
                "            }"
            ],
            [
                237,
                ""
            ],
            [
                249,
                "            Token currentToken = null;"
            ],
            [
                273,
                "                    Tokenize_Whitespace(input, ref currentIndex, ref currentCharacter, ref currentLine, ref currentColumn, ref currentToken, tokens);"
            ],
            [
                279,
                "                    Tokenize_EscapeCharacter(input, ref currentIndex, ref currentCharacter, ref currentLine, ref currentColumn, ref currentToken, tokens);"
            ],
            [
                283,
                "                    Tokenize_Plain(input, ref currentIndex, ref currentCharacter, ref currentLine, ref currentColumn, ref currentToken, tokens);"
            ],
            [
                287,
                "                    Tokenize_DelimiterCharacter(input, ref currentIndex, ref currentCharacter, ref currentLine, ref currentColumn, ref currentToken, tokens);"
            ],
            [
                291,
                "                    Tokenize_Whitespace(input, ref currentIndex, ref currentCharacter, ref currentLine, ref currentColumn, ref currentToken, tokens);"
            ],
            [
                295,
                "                    Tokenize_Plain(input, ref currentIndex, ref currentCharacter, ref currentLine, ref currentColumn, ref currentToken, tokens);"
            ],
            [
                328,
                "        private void Tokenize_EscapeCharacter(string input, ref int currentIndex, ref char currentCharacter, ref int line, ref int col, ref Token currentToken, List<T> tokens)"
            ],
            [
                343,
                "        private void Tokenize_Plain(string input, ref int currentIndex, ref char currentCharacter, ref int line, ref int col, ref Token currentToken, List<T> tokens)"
            ],
            [
                393,
                "                            currentToken = CreateTokenForTokenizer(delimiter, prevToken.StartIndex + prevToken.Value.Length, prevToken.Line, prevToken.Column + prevToken.Value.Length);"
            ],
            [
                401,
                "        private void Tokenize_DelimiterCharacter(string input, ref int currentIndex, ref char currentCharacter, ref int line, ref int col, ref Token currentToken, List<T> tokens)"
            ],
            [
                408,
                "        private void Tokenize_Whitespace(string input, ref int currentIndex, ref char currentCharacter, ref int line, ref int col, ref Token currentToken, List<T> tokens)"
            ],
            [
                430,
                "        private void FinalizeTokenIfNotNull(ref Token currentToken, List<T> tokens)"
            ],
            [
                434,
                "                tokens.Add(TokenFactoryImpl(currentToken, tokens));"
            ],
            [
                440,
                "        protected virtual T TokenFactoryImpl(Token current, List<T> tokens)"
            ],
            [
                441,
                "        {"
            ],
            [
                442,
                "            return TokenFactory(current, tokens);"
            ],
            [
                443,
                "        }"
            ],
            [
                444,
                ""
            ],
            [
                445,
                "        private void AppendToTokenSafe(ref Token currentToken, char toAppend, int startIndex, int line, int col)"
            ],
            [
                488,
                "        private Token CreateTokenForTokenizer(string currentCharacter, int currentIndex, int line, int col)"
            ],
            [
                490,
                "            var ret = new Token(currentCharacter, currentIndex, line, col);"
            ],
            [
                495,
                "        private Token CreateTokenForTokenizer(char currentCharacter,int currentIndex, int line, int col)"
            ],
            [
                497,
                "            return CreateTokenForTokenizer(currentCharacter + \"\", currentIndex, line, col);"
            ]
        ]
    },
    "num_lines_added": 20,
    "num_lines_removed": 50
}