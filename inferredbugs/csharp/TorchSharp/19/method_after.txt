        [TestMethod]
        public void TestTrainingAdam()
        {
            var lin1 = NN.Module.Linear(1000, 100);
            var lin2 = NN.Module.Linear(100, 10);
            var seq = NN.Module.Sequential(lin1, NN.Module.Relu(), lin2);

            var x = FloatTensor.RandomN(new long[] { 64, 1000 }, device: "cpu:0");
            var y = FloatTensor.RandomN(new long[] { 64, 10 }, device: "cpu:0");

            double learning_rate = 0.00004f;
            float prevLoss = float.MaxValue;
            var optimizer = NN.Optimizer.Adam(seq.Parameters(), learning_rate);

            for (int i = 0; i < 10; i++)
            {
                var eval = seq.Forward(x);
                var loss = NN.LossFunction.MSE(eval, y, NN.Reduction.Sum);
                var lossVal = loss.Item;

                Assert.IsTrue(lossVal < prevLoss);
                prevLoss = lossVal;

                optimizer.ZeroGrad();

                loss.Backward();

                optimizer.Step();
            }
        }