{"hash": "cbdc499ba656aa68f85304c978ddf09600862c62", "message": "*) adding many missing (File)?(Input|Output)Stream.close() calls to avoid \"Too many open files bug\".\n\ngit-svn-id: https://svn.berlios.de/svnroot/repos/yacy/trunk@90 6c8d7289-2bf4-0310-a012-ef5d649a1542", "file_num_lines": 674, "diff_parsed": {"added": [[167, "\t\tHashSet set = new HashSet();"], [168, "        BufferedReader br = null;"], [169, "\t\ttry {"], [170, "\t\t    br = new BufferedReader(new InputStreamReader(new FileInputStream(filename)));"], [171, "\t\t    String line;"], [172, "\t\t    while ((line = br.readLine()) != null) {"], [173, "\t\t\t\tline = line.trim();"], [174, "\t\t\t\tif ((line.length() > 0) && (!(line.startsWith(\"#\")))) set.add(line.trim().toLowerCase());"], [175, "\t\t    }"], [176, "\t\t    br.close();"], [177, "\t\t    serverLog.logInfo(\"PROXY\", \"read \" + setname + \" set from file \" + filename);"], [178, "\t\t} catch (IOException e) {"], [179, "\t\t} finally {"], [180, "            if (br != null) try { br.close(); } catch (Exception e) {}"], [181, "\t\t}"], [182, "\t\treturn set;"], [186, "\t\tTreeMap map = new TreeMap();"], [187, "        BufferedReader br = null;"], [188, "\t\ttry {"], [189, "\t\t    br = new BufferedReader(new InputStreamReader(new FileInputStream(filename)));"], [190, "\t\t    String line;"], [191, "\t\t    int pos;"], [192, "\t\t    while ((line = br.readLine()) != null) {"], [193, "\t\t\t\tline = line.trim();"], [194, "\t\t\t\tif ((line.length() > 0) && (!(line.startsWith(\"#\"))) && ((pos = line.indexOf(sep)) > 0))"], [195, "\t\t\t\t    map.put(line.substring(0, pos).trim().toLowerCase(), line.substring(pos + sep.length()).trim());"], [196, "\t\t    }"], [197, "\t\t    serverLog.logInfo(\"PROXY\", \"read \" + mapname + \" map from file \" + filename);"], [198, "\t\t} catch (IOException e) {"], [199, "\t\t} finally {"], [200, "            if (br != null) try { br.close(); } catch (Exception e) {}"], [201, "\t\t}"], [202, "\t\treturn map;"], [206, "\t\tTreeMap map = new TreeMap();"], [207, "\t\tif (switchboard == null) return map; // not initialized yet"], [208, "\t\tFile listsPath = new File(switchboard.getRootPath(), switchboard.getConfig(\"listsPath\", \"DATA/LISTS\"));"], [209, "\t\tString filenamesarray[] = filenames.split(\",\");"], [210, ""], [211, "\t\tif(filenamesarray.length >0)"], [212, "\t\t\tfor(int i = 0; i < filenamesarray.length; i++)"], [213, "\t\t\t\tmap.putAll(loadMap(mapname, (new File(listsPath, filenamesarray[i])).toString(), sep));"], [214, "\t\treturn map;"], [455, "                    InputStream is = null;"], [456, "                    try {"], [457, "\t                    is = new FileInputStream(cacheFile);"], [458, "\t                    byte[] buffer = new byte[2048];"], [459, "\t                    int l;"], [460, "\t                    while ((l = is.read(buffer)) > 0) {hfos.write(buffer, 0, l);}"], [461, "                    } finally {"], [462, "                        if (is != null) try { is.close(); } catch (Exception e) {}"], [463, "                    }"], [507, "\t\t\t\t// this is a file that is a possible candidate for parsing by the indexer"], [509, "\t\t\t\t\tlog.logDebug(\"create passthrough (parse candidate) for url \" + url);"], [510, "\t\t\t\t\t// no transformation, only passthrough"], [511, "\t\t\t\t\t// this is especially the case if the bluelist is empty"], [512, "\t\t\t\t\t// in that case, the content is not scraped here but later"], [516, "\t\t\t\t\tlog.logDebug(\"create scraper for url \" + url);"], [545, "\t\t\t\t\t\tlog.logDebug(\"writeContent of \" + url + \" produced cacheArray = \" + ((cacheArray == null) ? \"null\" : (\"size=\" + cacheArray.length)));"], [565, "\t\t\t\t\t\t// write to file right here."], [569, "\t\t\t\t\t\tlog.logDebug(\"for write-file of \" + url + \": contentLength = \" + contentLength + \", sizeBeforeDelete = \" + sizeBeforeDelete);"], [583, "\t\t\t\t\t\t// beware! all these writings will not fill the cacheEntry.cacheArray"], [584, "\t\t\t\t\t\t// that means they are not available for the indexer (except they are scraped before)"], [610, "\t\t\t\t// can have various reasons"], [612, "\t\t\t\tif (e.getMessage().indexOf(\"Corrupt GZIP trailer\") >= 0) {"], [613, "\t\t\t\t    // just do nothing, we leave it this way"], [614, "\t\t\t\t\tlog.logDebug(\"ignoring bad gzip trail for URL \" + url + \" (\" + e.getMessage() + \")\");"], [615, "\t\t\t\t} else {"], [616, "\t\t\t\t    respondHeader(respond,\"404 client unexpectedly closed connection\", new httpHeader(null));"], [617, "\t\t\t\t\tlog.logDebug(\"IOError for URL \" + url + \" (\" + e.getMessage() + \") - responded 404\");"], [618, "\t\t\t\t    e.printStackTrace();"], [619, "\t\t\t\t}"], [645, "        FileInputStream fis = null;"], [657, "            fis = new FileInputStream(file);"], [673, "        } catch (IOException e) {"], [674, "        } finally {"], [675, "\t\t\tif (fis != null) try { fis.close(); } catch (Exception e) {}"]], "deleted": [[167, "\tHashSet set = new HashSet();"], [168, "\ttry {"], [169, "\t    BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream(filename)));"], [170, "\t    String line;"], [171, "\t    while ((line = br.readLine()) != null) {"], [172, "\t\tline = line.trim();"], [173, "\t\tif ((line.length() > 0) && (!(line.startsWith(\"#\")))) set.add(line.trim().toLowerCase());"], [174, "\t    }"], [175, "\t    br.close();"], [176, "\t    serverLog.logInfo(\"PROXY\", \"read \" + setname + \" set from file \" + filename);"], [177, "\t} catch (IOException e) {}"], [178, "\treturn set;"], [182, "\tTreeMap map = new TreeMap();"], [183, "\ttry {"], [184, "\t    BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream(filename)));"], [185, "\t    String line;"], [186, "\t    int pos;"], [187, "\t    while ((line = br.readLine()) != null) {"], [188, "\t\tline = line.trim();"], [189, "\t\tif ((line.length() > 0) && (!(line.startsWith(\"#\"))) && ((pos = line.indexOf(sep)) > 0))"], [190, "\t\t    map.put(line.substring(0, pos).trim().toLowerCase(), line.substring(pos + sep.length()).trim());"], [191, "\t    }"], [192, "\t    br.close();"], [193, "\t    serverLog.logInfo(\"PROXY\", \"read \" + mapname + \" map from file \" + filename);"], [194, "\t} catch (IOException e) {}"], [195, "\treturn map;"], [199, "\tTreeMap map = new TreeMap();"], [200, "        if (switchboard == null) return map; // not initialized yet"], [201, "\tFile listsPath = new File(switchboard.getRootPath(), switchboard.getConfig(\"listsPath\", \"DATA/LISTS\"));"], [202, "        String filenamesarray[] = filenames.split(\",\");"], [203, "\tString filename = \"\";"], [204, "\tif(filenamesarray.length >0)"], [205, "\t    for(int i = 0; i < filenamesarray.length; i++)"], [206, "\t\tmap.putAll(loadMap(mapname, (new File(listsPath, filenamesarray[i])).toString(), sep));"], [207, "\treturn map;"], [448, "                    InputStream is = new FileInputStream(cacheFile);"], [449, "                    byte[] buffer = new byte[2048];"], [450, "                    int l;"], [451, "                    while ((l = is.read(buffer)) > 0) {hfos.write(buffer, 0, l);}"], [452, "                    is.close();"], [496, "\t\t// this is a file that is a possible candidate for parsing by the indexer"], [498, "\t\t    log.logDebug(\"create passthrough (parse candidate) for url \" + url);"], [499, "                    // no transformation, only passthrough"], [500, "\t\t    // this is especially the case if the bluelist is empty"], [501, "\t\t    // in that case, the content is not scraped here but later"], [505, "\t\t    log.logDebug(\"create scraper for url \" + url);"], [534, "\t\t\tlog.logDebug(\"writeContent of \" + url + \" produced cacheArray = \" + ((cacheArray == null) ? \"null\" : (\"size=\" + cacheArray.length)));"], [554, "\t\t\t// write to file right here."], [558, "\t\t\tlog.logDebug(\"for write-file of \" + url + \": contentLength = \" + contentLength + \", sizeBeforeDelete = \" + sizeBeforeDelete);"], [572, "\t\t\t// beware! all these writings will not fill the cacheEntry.cacheArray"], [573, "\t\t\t// that means they are not available for the indexer (except they are scraped before)"], [599, "\t\t// can have various reasons"], [601, "\t\tif (e.getMessage().indexOf(\"Corrupt GZIP trailer\") >= 0) {"], [602, "\t\t    // just do nothing, we leave it this way"], [603, "                    log.logDebug(\"ignoring bad gzip trail for URL \" + url + \" (\" + e.getMessage() + \")\");"], [604, "\t\t} else {"], [605, "\t\t    respondHeader(respond,\"404 client unexpectedly closed connection\", new httpHeader(null));"], [606, "                    log.logDebug(\"IOError for URL \" + url + \" (\" + e.getMessage() + \") - responded 404\");"], [607, "\t\t    e.printStackTrace();"], [608, "\t\t}"], [610, "            remote.close();"], [646, "            FileInputStream fis = new FileInputStream(file);"], [662, "        } catch (IOException e) {"], [663, ""]]}, "num_lines_added": 76, "num_lines_removed": 64}