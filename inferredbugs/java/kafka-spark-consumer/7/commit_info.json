{"hash": "a0f906a26eacf4e799fd6ee25316c2bc5efd0e5d", "message": "Control number of Receivers Task needed for your Partitions.\nEarlier version uses as many Receiver for number of partition.\nNow you can configure desired number of Receivers task and every Receiver handle subset of topic partitions.", "file_num_lines": 50, "diff_parsed": {"added": [[33, "import consumer.kafka.ReceiverLauncher;"], [38, ""], [47, ""], [48, "\t\tProperties props = new Properties();"], [49, "\t\tprops.put(\"zookeeper.hosts\", \"10.252.5.131\");"], [50, "\t\tprops.put(\"zookeeper.port\", \"2181\");"], [51, "\t\tprops.put(\"zookeeper.broker.path\", \"/brokers\");"], [52, "\t\tprops.put(\"kafka.topic\", \"valid_subpub\");"], [53, "\t\tprops.put(\"kafka.consumer.id\", \"valid_subpub\");"], [54, "\t\tprops.put(\"zookeeper.consumer.connection\", \"10.252.5.113:2182\");"], [55, "\t\tprops.put(\"zookeeper.consumer.path\", \"/kafka-new\");"], [56, ""], [58, "\t\t\t\t.set(\"spark.streaming.receiver.writeAheadLog.enable\", \"false\");;"], [62, ""], [63, "\t\t//Specify number of Receivers you need."], [64, "\t\t//It should be less than or equal to number of Partitions of your topic"], [65, ""], [66, "\t\tint numberOfReceivers = 2;"], [67, ""], [68, "\t\tJavaDStream<MessageAndMetadata> unionStreams = ReceiverLauncher.launch(ssc, props, numberOfReceivers);"], [69, ""], [70, "\t\tunionStreams"], [71, "\t\t\t\t.foreachRDD(new Function2<JavaRDD<MessageAndMetadata>, Time, Void>() {"], [72, ""], [73, "\t\t\t\t\t@Override"], [74, "\t\t\t\t\tpublic Void call(JavaRDD<MessageAndMetadata> rdd,"], [75, "\t\t\t\t\t\t\tTime time) throws Exception {"], [76, ""], [77, "\t\t\t\t\t\tSystem.out.println(\"Number of records in this Batch is \" + rdd.count());"], [78, "\t\t\t\t\t\treturn null;"], [79, "\t\t\t\t\t}"], [80, "\t\t\t\t});"], [81, ""]], "deleted": [[21, "import java.io.File;"], [22, "import java.io.FileInputStream;"], [24, "import java.util.ArrayList;"], [25, "import java.util.List;"], [28, "import org.apache.commons.cli.CommandLine;"], [29, "import org.apache.commons.cli.CommandLineParser;"], [30, "import org.apache.commons.cli.OptionBuilder;"], [31, "import org.apache.commons.cli.Options;"], [32, "import org.apache.commons.cli.PosixParser;"], [41, "import consumer.kafka.KafkaConfig;"], [47, "\tprivate Properties _props;"], [48, "\tprivate KafkaConfig _kafkaConfig;"], [49, ""], [53, "\t\t_kafkaConfig = new KafkaConfig(_props);"], [57, "\tprivate void init(String[] args) throws Exception {"], [58, ""], [59, "\t\tOptions options = new Options();"], [60, "\t\tthis._props = new Properties();"], [61, ""], [62, "\t\toptions.addOption(\"p\", true, \"properties filename from the classpath\");"], [63, "\t\toptions.addOption(\"P\", true, \"external properties filename\");"], [64, ""], [65, "\t\tOptionBuilder.withArgName(\"property=value\");"], [66, "\t\tOptionBuilder.hasArgs(2);"], [67, "\t\tOptionBuilder.withValueSeparator();"], [68, "\t\tOptionBuilder.withDescription(\"use value for given property\");"], [69, "\t\toptions.addOption(OptionBuilder.create(\"D\"));"], [70, ""], [71, "\t\tCommandLineParser parser = new PosixParser();"], [72, "\t\tCommandLine cmd = parser.parse(options, args);"], [73, "\t\tif (cmd.hasOption('p')) {"], [74, "\t\t\tthis._props.load(ClassLoader.getSystemClassLoader()"], [75, "\t\t\t\t\t.getResourceAsStream(cmd.getOptionValue('p')));"], [76, "\t\t}"], [77, "\t\tif (cmd.hasOption('P')) {"], [78, "\t\t\tFile file = new File(cmd.getOptionValue('P'));"], [79, "\t\t\tFileInputStream fStream = new FileInputStream(file);"], [80, "\t\t\tthis._props.load(fStream);"], [81, "\t\t}"], [82, "\t\tthis._props.putAll(cmd.getOptionProperties(\"D\"));"], [83, ""], [84, "\t}"], [85, ""], [88, "\t\tString checkpointDirectory = \"hdfs://10.252.5.113:9000/user/hadoop/spark\";"], [89, "\t\tint _partitionCount = 3;"], [90, ""], [91, "\t\tList<JavaDStream<MessageAndMetadata>> streamsList = new ArrayList<JavaDStream<MessageAndMetadata>>("], [92, "\t\t\t\t_partitionCount);"], [93, "\t\tJavaDStream<MessageAndMetadata> unionStreams;"], [94, ""], [96, "\t\t\t\t.set(\"spark.streaming.receiver.writeAheadLog.enable\", \"true\");;"], [100, ""], [101, "\t\tfor (int i = 0; i < _partitionCount; i++) {"], [102, ""], [103, "\t\t\tstreamsList.add(ssc.receiverStream(new KafkaReceiver(_props, i)));"], [104, ""], [105, "\t\t}"], [106, ""], [107, "\t\t// Union all the streams if there is more than 1 stream"], [108, "\t\tif (streamsList.size() > 1) {"], [109, "\t\t\tunionStreams = ssc.union(streamsList.get(0),"], [110, "\t\t\t\t\tstreamsList.subList(1, streamsList.size()));"], [111, "\t\t} else {"], [112, "\t\t\t// Otherwise, just use the 1 stream"], [113, "\t\t\tunionStreams = streamsList.get(0);"], [114, "\t\t}"], [115, ""], [116, "\t\tunionStreams.checkpoint(new Duration(10000));"], [117, ""], [118, "\t\ttry {"], [119, "\t\t\tunionStreams"], [120, "\t\t\t\t\t.foreachRDD(new Function2<JavaRDD<MessageAndMetadata>, Time, Void>() {"], [121, ""], [122, "\t\t\t\t\t\t@Override"], [123, "\t\t\t\t\t\tpublic Void call(JavaRDD<MessageAndMetadata> rdd,"], [124, "\t\t\t\t\t\t\t\tTime time) throws Exception {"], [125, ""], [126, "\t\t\t\t\t\t\tfor (MessageAndMetadata record : rdd.collect()) {"], [127, ""], [128, "\t\t\t\t\t\t\t\tif (record != null) {"], [129, ""], [130, "\t\t\t\t\t\t\t\t\tSystem.out.println(new String(record"], [131, "\t\t\t\t\t\t\t\t\t\t\t.getPayload()));"], [132, ""], [133, "\t\t\t\t\t\t\t\t}"], [134, ""], [135, "\t\t\t\t\t\t\t}"], [136, "\t\t\t\t\t\t\treturn null;"], [137, "\t\t\t\t\t\t}"], [138, "\t\t\t\t\t});"], [139, "\t\t} catch (Exception ex) {"], [140, ""], [141, "\t\t\tex.printStackTrace();"], [142, "\t\t}"], [143, ""], [144, "\t\tssc.checkpoint(checkpointDirectory);"], [152, "\t\tconsumer.init(args);"]]}, "num_lines_added": 33, "num_lines_removed": 97}