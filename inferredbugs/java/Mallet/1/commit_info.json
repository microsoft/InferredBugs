{"hash": "b24803457088ba5c421a8d9ec3f72c6e0e59bd3c", "message": "two instance lists", "file_num_lines": 544, "diff_parsed": {"added": [[11, "import java.util.List;"], [12, "import java.util.ArrayList;"], [13, ""], [40, "\t// Instance list containing word features"], [41, "\tInstanceList instances = null;"], [42, ""], [43, "\t// Instance list containing topic features"], [44, "\tInstanceList topics = null;"], [46, "\t// Instance list for empirical likelihood calculation"], [47, "\tInstanceList testing = null;"], [48, ""], [49, "\t// An array to put the topic counts for"], [50, "\t//  the current document. Defined here to avoid"], [51, "\t//   garbage collection overhead."], [53, ""], [89, "\t\tSystem.out.println(\"LDA: \" + numberOfTopics + \" topics\");"], [94, ""], [98, "\t\trandom = new Randoms();"], [99, ""], [100, "\t\toneDocTopicCounts = new int[numTopics];"], [101, "\t\ttokensPerTopic = new int[numTopics];"], [102, "\t\ttopicWeights = new double[numTopics];"], [103, ""], [147, "\t/** Set the instance list with word features for training */"], [148, "\tpublic void addInstances(InstanceList training) {"], [149, "\t\tthis.instances = training;"], [150, ""], [151, "\t\t// We need to create a new instance list."], [152, "\t\t//  Since the topics aren't really meaningful on their own,"], [153, "\t\t// just add placeholders in the data alphabet."], [155, "\t\tAlphabet topicAlphabet = new Alphabet();"], [157, "\t\tSystem.out.println(\"adding alphabet \" + topicAlphabet);"], [158, ""], [159, "\t\tfor (int topic=0; topic < numTopics; topic++) {"], [160, "\t\t\ttopicAlphabet.lookupIndex(\"Topic \" + topic, true);"], [163, "\t\tthis.topics = new InstanceList(topicAlphabet, null);"], [164, ""], [165, "\t\tnumTypes = instances.getDataAlphabet().size ();"], [166, "\t\ttypeTopicCounts = new int[numTypes][numTopics];"], [167, "\t\tbetaSum = beta * numTypes;"], [168, ""], [169, "\t\tList<Instance> rawTopicInstances = new ArrayList<Instance>();"], [170, ""], [171, "\t\tfor (Instance instance: instances) {"], [172, "\t\t\tFeatureSequence tokenSequence = (FeatureSequence) instance.getData();"], [173, "\t\t\trawTopicInstances.add(new Instance( new FeatureSequence( topicAlphabet,"], [174, "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t new int[tokenSequence.getLength()] ),"], [175, "\t\t\t\t\t\t\t\t\t\t\t\tnull, null, null ));"], [176, "\t\t}"], [177, ""], [178, "\t\t// Run the instances through the pipe"], [179, ""], [180, "\t\ttopics.addThruPipe(rawTopicInstances.iterator());"], [181, ""], [182, "\t\tfor (int doc=0; doc < instances.size(); doc++) {"], [183, "\t\t\tFeatureSequence tokenSequence = (FeatureSequence) instances.get(doc).getData();"], [184, "\t\t\tFeatureSequence topicSequence = (FeatureSequence) topics.get(doc).getData();"], [185, ""], [186, "\t\t\t// Sample topics according to the predictive distribution"], [187, "\t\t\tsampleTopicsForOneDoc(tokenSequence, topicSequence,"], [188, "\t\t\t\t\t\t\t\t  false, true);\t//   (don't store histograms, do start from nothing)"], [189, "\t\t}"], [190, ""], [191, "\t\tinitializeHistograms();"], [192, "\t}"], [193, ""], [194, "\tpublic void addInstances(InstanceList training, InstanceList topics) {"], [195, "\t\tthis.instances = training;"], [196, "\t\tthis.topics = topics;"], [197, ""], [202, "\t\tfor (int doc = 0; doc < instances.size(); doc++) {"], [203, ""], [204, "\t\t\tFeatureSequence tokenSequence = (FeatureSequence) instances.get(doc).getData();"], [205, "\t\t\tFeatureSequence topicSequence = (FeatureSequence) topics.get(doc).getData();"], [206, ""], [207, "\t\t\tfor (int token = 0; token < topicSequence.getLength(); token++) {"], [208, "\t\t\t\tint topic = topicSequence.getIndexAtPosition(token);"], [209, "\t\t\t\ttypeTopicCounts[ tokenSequence.getIndexAtPosition(token) ][topic]++;"], [210, "\t\t\t\ttokensPerTopic[topic]++;"], [211, "\t\t\t}"], [212, "\t\t}"], [213, ""], [214, "\t\tinitializeHistograms();"], [215, "\t}"], [216, ""], [217, "\t/**"], [218, "\t *  Gather statistics on the size of documents"], [219, "\t *  and create histograms for use in Dirichlet hyperparameter"], [220, "\t *  optimization."], [221, "\t */"], [222, "\tprivate void initializeHistograms() {"], [227, ""], [228, "\t\tfor (int doc = 0; doc < instances.size(); doc++) {"], [285, "\t\t\tfor (int doc = 0; doc < topics.size(); doc++) {"], [286, ""], [287, "\t\t\t\tFeatureSequence tokenSequence = (FeatureSequence) instances.get(doc).getData();"], [288, "\t\t\t\tFeatureSequence topicSequence = (FeatureSequence) topics.get(doc).getData();"], [289, ""], [290, "\t\t\t\tsampleTopicsForOneDoc (tokenSequence, topicSequence,"], [292, "\t\t\t\t\t\t\t\t\t   iterations % saveSampleInterval == 0,"], [293, "\t\t\t\t\t\t\t\t\t   false);"], [323, "\tprivate void sampleTopicsForOneDoc (FeatureSequence tokenSequence,"], [324, "\t\t\t\t\t\t\t\t\t\tFeatureSequence topicSequence,"], [325, "\t\t\t\t\t\t\t\t\t\tboolean shouldSaveState, boolean initializing) {"], [329, "\t\tint[] oneDocTopics = topicSequence.getFeatures();"], [334, "\t\tint docLen = tokenSequence.getLength();"], [340, ""], [341, "\t\tif (! initializing) {"], [342, "\t\t\tfor (int token = 0; token < docLen; token++) {"], [343, "\t\t\t\toneDocTopicCounts[ oneDocTopics[token] ]++;"], [344, "\t\t\t}"], [349, "\t\t\ttype = tokenSequence.getIndexAtPosition(token);"], [352, "\t\t\tif (! initializing) {"], [353, "\t\t\t\t// Remove this token from all counts"], [354, "\t\t\t\toneDocTopicCounts[oldTopic]--;"], [355, "\t\t\t\ttypeTopicCounts[type][oldTopic]--;"], [356, "\t\t\t\ttokensPerTopic[oldTopic]--;"], [357, "\t\t\t}"], [466, "\t\tfor (int doc = 0; doc < topics.size(); doc++) {"], [467, "\t\t\tFeatureSequence topicSequence ="], [468, "\t\t\t\t(FeatureSequence) topics.get(doc).getData();"], [469, "\t\t\tint[] currentDocTopics = topicSequence.getFeatures();"], [470, ""], [473, "\t\t\tif (instances.get(doc).getSource() != null) {"], [481, "\t\t\tdocLen = currentDocTopics.length;"], [485, "\t\t\t\ttopicCounts[ currentDocTopics[token] ]++;"], [502, ""], [503, "\t\t\tpw.print(output.toString());"], [504, "\t\t\toutput = new StringBuffer();"], [517, "\t\tAlphabet alphabet = instances.getDataAlphabet();"], [518, ""], [520, "\t\tfor (int doc = 0; doc < topics.size(); doc++) {"], [521, ""], [522, "\t\t\tFeatureSequence tokenSequence ="], [523, "\t\t\t\t(FeatureSequence) instances.get(doc).getData();"], [524, "\t\t\tFeatureSequence topicSequence ="], [525, "\t\t\t\t(FeatureSequence) instances.get(doc).getData();"], [526, ""], [527, "\t\t\tfor (int token = 0; token < topicSequence.getLength(); token++) {"], [528, "\t\t\t\tint type = tokenSequence.getIndexAtPosition(token);"], [529, "\t\t\t\tint topic = topicSequence.getIndexAtPosition(token);"], [530, "\t\t\t\tout.print(doc); out.print(' ');"], [533, "\t\t\t\tout.print(alphabet.lookupObject(type)); out.print(' ');"], [534, "\t\t\t\tout.print(topic); out.println();"], [559, ""], [560, "\t\t// Instance lists"], [562, "\t\tout.writeObject (topics);"], [563, ""], [568, ""], [572, ""], [580, ""], [582, "\t\ttopics = (InstanceList) in.readObject ();"], [583, ""], [589, ""], [612, ""], [613, "\t\t\tFeatureSequence topicSequence ="], [614, "\t\t\t\t(FeatureSequence) topics.get(doc).getData();"], [615, "\t\t\tdocTopics = topicSequence.getFeatures();"], [773, "\t\tfor (int doc=0; doc < topics.size(); doc++) {"], [774, "\t\t\tFeatureSequence topicSequence ="], [775, "\t\t\t\t(FeatureSequence) topics.get(doc).getData();"], [776, ""], [777, "\t\t\tdocTopics = topicSequence.getFeatures();"], [797, "\t\tlogLikelihood += topics.size() * Dirichlet.logGammaStirling(alphaSum);"], [840, "\t\tlda.addInstances(training);"]], "deleted": [[29, "\tint numTokens;"], [38, "\tInstanceList instances, testing;  // the data field of the instances is expected to hold a FeatureSequence"], [40, "\tint[][] topics; // indexed by <document index, sequence index>"], [66, "\tRuntime runtime;"], [78, "\t\tSystem.out.println(\"optimizingLDA: \" + numberOfTopics);"], [86, "\t\truntime = Runtime.getRuntime();"], [89, "\tpublic void setTrainingInstances(InstanceList training) {"], [90, "\t\tthis.instances = training;"], [91, "\t}"], [92, ""], [135, "\t/** Set up arrays and pick random topics */"], [136, "\tpublic void initialize() {"], [138, "\t\tif (random == null) {"], [139, "\t\t\trandom = new Randoms();"], [143, "\t\tint numDocs = instances.size();"], [144, "\t\ttopics = new int[numDocs][];"], [145, "\t\toneDocTopicCounts = new int[numTopics];"], [147, "\t\ttokensPerTopic = new int[numTopics];"], [150, "\t\ttopicWeights = new double[numTopics];"], [154, ""], [155, "\t\t// Initialize with random assignments of tokens to topics"], [156, "\t\t// and finish allocating this.topics and this.tokens"], [158, "\t\tfor (int doc = 0; doc < numDocs; doc++) {"], [165, ""], [166, "\t\t\tnumTokens += seqLen;"], [167, "\t\t\ttopics[doc] = new int[seqLen];"], [168, "\t\t\t// Randomly assign tokens to topics"], [169, "\t\t\tfor (int token = 0; token < seqLen; token++) {"], [170, "\t\t\t\ttopic = random.nextInt(numTopics);"], [171, "\t\t\t\ttopics[doc][token] = topic;"], [172, ""], [173, "\t\t\t\ttypeTopicCounts[ fs.getIndexAtPosition(token) ][topic]++;"], [174, "\t\t\t\ttokensPerTopic[topic]++;"], [175, "\t\t\t}"], [181, "\t\t// These will be initialized at the first call to"], [182, "\t\t//\tclearHistograms() in the loop below."], [228, "\t\t\tfor (int doc = 0; doc < topics.length; doc++) {"], [229, "\t\t\t\tsampleTopicsForOneDoc (doc,"], [231, "\t\t\t\t\t\t\t\t\t   iterations % saveSampleInterval == 0);"], [261, "\tprivate void sampleTopicsForOneDoc (int doc, boolean shouldSaveState) {"], [265, "\t\tFeatureSequence tokens = (FeatureSequence) instances.get(doc).getData();"], [266, "\t\tint[] oneDocTopics = topics[doc];"], [271, "\t\tint docLen = tokens.getLength();"], [277, "\t\tfor (int token = 0; token < docLen; token++) {"], [278, "\t\t\toneDocTopicCounts[ oneDocTopics[token] ]++;"], [283, "\t\t\ttype = tokens.getIndexAtPosition(token);"], [286, "\t\t\t// Remove this token from all counts"], [287, "\t\t\toneDocTopicCounts[oldTopic]--;"], [288, "\t\t\ttypeTopicCounts[type][oldTopic]--;"], [289, "\t\t\ttokensPerTopic[oldTopic]--;"], [398, "\t\tfor (int doc = 0; doc < topics.length; doc++) {"], [401, "\t\t\tif (instances.get(doc).getSource() != null){"], [409, "\t\t\tdocLen = topics[doc].length;"], [413, "\t\t\t\ttopicCounts[ topics[doc][token] ]++;"], [432, "\t\tpw.print(output.toString());"], [443, "\t\tAlphabet a = instances.getDataAlphabet();"], [445, "\t\tfor (int di = 0; di < topics.length; di++) {"], [446, "\t\t\tFeatureSequence fs = (FeatureSequence) instances.get(di).getData();"], [447, "\t\t\tfor (int token = 0; token < topics[di].length; token++) {"], [448, "\t\t\t\tint type = fs.getIndexAtPosition(token);"], [449, "\t\t\t\tout.print(di); out.print(' ');"], [452, "\t\t\t\tout.print(a.lookupObject(type)); out.print(' ');"], [453, "\t\t\t\tout.print(topics[di][token]); out.println();"], [483, "\t\tfor (int di = 0; di < topics.length; di ++)"], [484, "\t\t\tfor (int si = 0; si < topics[di].length; si++)"], [485, "\t\t\t\tout.writeInt (topics[di][si]);"], [486, "\t\t/*for (int di = 0; di < topics.length; di ++)"], [487, "\t\t  for (int ti = 0; ti < numTopics; ti++)"], [488, "\t\t  out.writeInt (docTopicCounts[di][ti]);*/"], [505, "\t\ttopics = new int[numDocs][];"], [506, "\t\tfor (int di = 0; di < instances.size(); di++) {"], [507, "\t\t\tint docLen = ((FeatureSequence)instances.get(di).getData()).getLength();"], [508, "\t\t\ttopics[di] = new int[docLen];"], [509, "\t\t\tfor (int si = 0; si < docLen; si++)"], [510, "\t\t\t\ttopics[di][si] = in.readInt();"], [511, "\t\t}"], [512, "\t\t/*"], [513, "\t\t  docTopicCounts = new int[numDocs][numTopics];"], [514, "\t\t  for (int di = 0; di < instances.size(); di++)"], [515, "\t\t  for (int ti = 0; ti < numTopics; ti++)"], [516, "\t\t  docTopicCounts[di][ti] = in.readInt();"], [517, "\t\t*/"], [540, "\t\t\tdocTopics = topics[doc];"], [698, "\t\tfor (int doc=0; doc < topics.length; doc++) {"], [699, ""], [700, "\t\t\tdocTopics = topics[doc];"], [720, "\t\tlogLikelihood += topics.length * Dirichlet.logGammaStirling(alphaSum);"], [763, "\t\tlda.setTrainingInstances(training);"], [764, "\t\t//lda.setTopicDisplayInterval(1);"], [765, ""], [766, "\t\tlda.initialize();"]]}, "num_lines_added": 165, "num_lines_removed": 91}