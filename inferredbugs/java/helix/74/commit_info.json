{"hash": "ac2b715400f02c9735b21bc46bd7a7be51e789c5", "message": "Implement quota based workflow queues for existing pipeline\n\nThe first step for task assignment witb producer/consumer mode is to have quota based workflow queues. For this ticket, it tracks the implementation for these queues to hold the order of workflows.", "file_num_lines": 222, "diff_parsed": {"added": [[3, "import com.google.common.collect.Maps;"], [4, "import java.util.ArrayList;"], [5, "import java.util.HashMap;"], [7, "import java.util.List;"], [29, "import org.apache.helix.task.WorkflowConfig;"], [31, "import org.apache.helix.task.WorkflowDispatcher;"], [32, "import org.apache.helix.task.assigner.AssignableInstance;"], [39, "  private Map<String, PriorityQueue<WorkflowObject>> _quotaBasedWorkflowPQs = Maps.newHashMap();"], [40, "  private WorkflowDispatcher _workflowDispatcher;"], [59, "    buildQuotaBasedWorkflowPQsAndInitDispatchers(cache,"], [60, "        (HelixManager) event.getAttribute(AttributeName.helixmanager.name()),"], [61, "        (ClusterStatusMonitor) event.getAttribute(AttributeName.clusterStatusMonitor.name()));"], [62, ""], [72, "    Map<String, Resource> restOfResources = new HashMap<>(resourceMap);"], [74, "    final List<String> failureResources = new ArrayList<>();"], [75, "    // Queues only for Workflows"], [76, "    scheduleWorkflows(resourceMap, cache, restOfResources, failureResources);"], [78, "    // Current rest of resources including: jobs + only current state left over ones"], [79, "    Iterator<Resource> itr = restOfResources.values().iterator();"], [80, "    while (itr.hasNext()) {"], [81, "      Resource resource = itr.next();"], [83, "        failureResources.add(resource.getResourceName());"], [84, "        LogUtil.logWarn(logger, _eventId,"], [85, "            \"Failed to calculate best possible states for \" + resource.getResourceName());"], [163, "        updateOutput(resource, partitionStateAssignment, output);"], [189, "  class WorkflowObject implements Comparable<WorkflowObject> {"], [190, "    String _workflowId;"], [191, "    Long _rankingValue;"], [192, ""], [193, "    public WorkflowObject(String workflowId, long rankingValue) {"], [194, "      _workflowId = workflowId;"], [195, "      _rankingValue = rankingValue;"], [196, "    }"], [198, "    @Override"], [199, "    public int compareTo(WorkflowObject o) {"], [200, "      return (int) (_rankingValue - o._rankingValue);"], [202, "  }"], [204, "  private void buildQuotaBasedWorkflowPQsAndInitDispatchers(ClusterDataCache cache, HelixManager manager,"], [205, "      ClusterStatusMonitor monitor) {"], [206, "    _quotaBasedWorkflowPQs.clear();"], [207, "    Map<String, String> quotaRatioMap = cache.getClusterConfig().getTaskQuotaRatioMap();"], [209, "    // Create quota based queues"], [210, "    if (quotaRatioMap == null || quotaRatioMap.size() == 0) {"], [211, "      _quotaBasedWorkflowPQs"], [212, "          .put(AssignableInstance.DEFAULT_QUOTA_TYPE, new PriorityQueue<WorkflowObject>());"], [213, "    } else {"], [214, "      for (String quotaType : quotaRatioMap.keySet()) {"], [215, "        _quotaBasedWorkflowPQs.put(quotaType, new PriorityQueue<WorkflowObject>());"], [219, "    for (String workflowId : cache.getWorkflowConfigMap().keySet()) {"], [220, "      WorkflowConfig workflowConfig = cache.getWorkflowConfig(workflowId);"], [221, "      String workflowType = workflowConfig.getWorkflowType();"], [222, "      if (workflowType == null || !_quotaBasedWorkflowPQs.containsKey(workflowType)) {"], [223, "        workflowType = AssignableInstance.DEFAULT_QUOTA_TYPE;"], [224, "      }"], [225, "      // TODO: We can support customized sorting field for user. Currently sort by creation time"], [226, "      _quotaBasedWorkflowPQs.get(workflowType)"], [227, "          .add(new WorkflowObject(workflowId, workflowConfig.getRecord().getCreationTime()));"], [228, "    }"], [229, "    if (_workflowDispatcher == null) {"], [230, "      _workflowDispatcher = new WorkflowDispatcher();"], [231, "    }"], [232, "    _workflowDispatcher.init(manager);"], [233, "    _workflowDispatcher.setClusterStatusMonitor(monitor);"], [234, "    _workflowDispatcher.updateCache(cache.getTaskDataCache());"], [235, "  }"], [236, ""], [237, "  private void updateOutput(Resource resource, ResourceAssignment partitionStateAssignment,"], [238, "      BestPossibleStateOutput output) {"], [239, "    // Use the internal MappingCalculator interface to compute the final assignment"], [240, "    // The next release will support rebalancers that compute the mapping from start to finish"], [241, "    for (Partition partition : resource.getPartitions()) {"], [242, "      Map<String, String> newStateMap = partitionStateAssignment.getReplicaMap(partition);"], [243, "      output.setState(resource.getResourceName(), partition, newStateMap);"], [244, "    }"], [245, "  }"], [246, ""], [247, "  private void scheduleWorkflows(Map<String, Resource> resourceMap, ClusterDataCache cache,"], [248, "      Map<String, Resource> restOfResources, List<String> failureResources) {"], [249, "    for (PriorityQueue<WorkflowObject> quotaBasedWorkflowPQ : _quotaBasedWorkflowPQs.values()) {"], [250, "      Iterator<WorkflowObject> it = quotaBasedWorkflowPQ.iterator();"], [251, "      while (it.hasNext()) {"], [252, "        String workflowId = it.next()._workflowId;"], [253, "        Resource resource = resourceMap.get(workflowId);"], [254, "        // TODO : Resource is null could be workflow just created without any IdealState."], [255, "        // Let's remove this check when Helix is independent from IdealState"], [256, "        if (resource != null) {"], [257, "          try {"], [258, "            WorkflowContext context = _workflowDispatcher"], [259, "                .getOrInitializeWorkflowContext(workflowId, cache.getTaskDataCache());"], [260, "            _workflowDispatcher"], [261, "                .updateWorkflowStatus(workflowId, cache.getWorkflowConfig(workflowId), context);"], [262, "            _workflowDispatcher"], [263, "                .assignWorkflow(workflowId, cache.getWorkflowConfig(workflowId), context);"], [264, "            restOfResources.remove(workflowId);"], [265, "          } catch (Exception e) {"], [266, "            LogUtil.logError(logger, _eventId,"], [267, "                \"Error computing assignment for Workflow \" + workflowId + \". Skipping.\", e);"], [268, "            failureResources.add(workflowId);"], [269, "          }"], [270, "        }"], [271, "      }"]], "deleted": [[23, "import org.apache.helix.task.JobContext;"], [24, "import org.apache.helix.task.JobRebalancer;"], [26, "import org.apache.helix.task.TaskDriver;"], [64, "    PriorityQueue<TaskSchedulingStage.ResourcePriority> resourcePriorityQueue ="], [65, "        new PriorityQueue<>();"], [66, "    TaskDriver taskDriver = null;"], [67, "    HelixManager helixManager = event.getAttribute(AttributeName.helixmanager.name());"], [68, "    if (helixManager != null) {"], [69, "      taskDriver = new TaskDriver(helixManager);"], [70, "    }"], [71, "    for (Resource resource : resourceMap.values()) {"], [72, "      resourcePriorityQueue.add(new TaskSchedulingStage.ResourcePriority(resource,"], [73, "          cache.getIdealState(resource.getResourceName()), taskDriver));"], [74, "    }"], [75, ""], [76, "    // TODO: Replace this looping available resources with Workflow Queues"], [77, "    for (Iterator<TaskSchedulingStage.ResourcePriority> itr = resourcePriorityQueue.iterator();"], [78, "        itr.hasNext(); ) {"], [79, "      Resource resource = itr.next().getResource();"], [81, "        LogUtil"], [82, "            .logWarn(logger, _eventId, \"Failed to assign tasks for \" + resource.getResourceName());"], [158, ""], [159, "      // Use the internal MappingCalculator interface to compute the final assignment"], [160, "      // The next release will support rebalancers that compute the mapping from start to finish"], [163, "        for (Partition partition : resource.getPartitions()) {"], [164, "          Map<String, String> newStateMap = partitionStateAssignment.getReplicaMap(partition);"], [165, "          output.setState(resourceName, partition, newStateMap);"], [166, "        }"], [192, "  class ResourcePriority implements Comparable<ResourcePriority> {"], [193, "    final Resource _resource;"], [194, "    // By default, non-job resources and new jobs are assigned lowest priority"], [195, "    Long _priority = Long.MAX_VALUE;"], [197, "    Resource getResource() {"], [198, "      return _resource;"], [201, "    public ResourcePriority(Resource resource, IdealState idealState, TaskDriver taskDriver) {"], [202, "      _resource = resource;"], [204, "      if (taskDriver != null && idealState != null"], [205, "          && idealState.getRebalancerClassName() != null"], [206, "          && idealState.getRebalancerClassName().equals(JobRebalancer.class.getName())) {"], [207, "        // Update priority for job resources, note that older jobs will be processed earlier"], [208, "        JobContext jobContext = taskDriver.getJobContext(resource.getResourceName());"], [209, "        if (jobContext != null && jobContext.getStartTime() != WorkflowContext.UNSTARTED) {"], [210, "          _priority = jobContext.getStartTime();"], [211, "        }"], [215, "    @Override"], [216, "    public int compareTo(ResourcePriority otherJob) {"], [217, "      return _priority.compareTo(otherJob._priority);"]]}, "num_lines_added": 101, "num_lines_removed": 47}